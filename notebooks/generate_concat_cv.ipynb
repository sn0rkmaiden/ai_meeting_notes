{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "314dde4f-28c0-4ca9-aa64-517167af4678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lemon/Documents/workspace/ai_meeting_notes/.venv/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyannote.audio import Pipeline\n",
    "import random\n",
    "from pathlib import Path\n",
    "from hashlib import md5\n",
    "import gzip\n",
    "import shutil\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import torch\n",
    "from torchmetrics.text import WordErrorRate\n",
    "from IPython.display import Audio\n",
    "import torchaudio.functional as audio_func\n",
    "from pyannote.core import notebook, Annotation, Segment, Timeline\n",
    "from pyannote.metrics.segmentation import SegmentationCoverage, SegmentationPurity, SegmentationPrecision, SegmentationRecall\n",
    "from pyannote.metrics.identification import IdentificationPrecision, IdentificationRecall, IdentificationErrorRate\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk import word_tokenize\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "babd26cb-cc42-4f7d-966f-3b1164280433",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d9d4bbe-2bd4-41e8-9085-88fef6038d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.environ[\"HUGGING_FACE_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f18eaf1c-6b28-4110-a14d-d74d617f38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_sample_rate = 16000\n",
    "min_upvote_count = 2\n",
    "output_path = Path(\"data/dev/concat-cv\")\n",
    "output_count = 10\n",
    "clients_per_output = 5\n",
    "output_sample_count = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e644d648-d8e9-4f79-8fa6-8da1b636cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e6f64e5-25fa-4b4b-9653-61d67e7ecc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_path = Path(\"data/cv-corpus-21.0-2025-03-14/ru/\")\n",
    "clips_path = dataset_base_path / \"clips\"\n",
    "test_csv_path = dataset_base_path / \"dev.tsv\"\n",
    "invalidated_csv_path = dataset_base_path / \"invalidated.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e80cd5e-e847-4053-a13b-e0aaccd14c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = pd.read_csv(test_csv_path.absolute(), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c0f6f75-8db1-45b6-94bc-96de2c8bcf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalidated = pd.read_csv(invalidated_csv_path, sep=\"\\t\")\n",
    "invalidated_ids = set(invalidated[\"client_id\"])\n",
    "files = files[~files[\"client_id\"].isin(invalidated_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a3f9b27-2141-45c7-a9b7-d07c74cc03b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce GTX 1050 Ti is available.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available. Training will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "596e4441-a222-4eae-bff4-98f362af1b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "/home/lemon/Documents/workspace/ai_meeting_notes/.venv/lib64/python3.12/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.1.3 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/059e96f964841d40f1a5e755bb7223f76666bba4/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.7.1, yours is 2.6.0+cu124. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "vad = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\", use_auth_token=hf_token).to(device)\n",
    "initial_params = {\"onset\": 0.4, \"offset\": 0.3, \"min_duration_on\": 0.0, \"min_duration_off\": 0.1}\n",
    "vad.instantiate(initial_params);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df9737ba-2d77-48dd-b5b9-0aa7159770ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file, target_sample_rate=default_sample_rate, **kwargs):\n",
    "    tensor, sample_rate = torchaudio.load(file)\n",
    "    tensor /= tensor.abs().max()\n",
    "    return audio_func.resample(tensor, sample_rate, target_sample_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2503778d-f4e8-4c19-bb32-17be0caf746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_sample(tensor, sample_rate=default_sample_rate):\n",
    "    activity = vad({\"waveform\": tensor, \"sample_rate\": sample_rate})\n",
    "    if len(activity.get_timeline()) == 0:\n",
    "        return None\n",
    "    start = activity.get_timeline()[0].start\n",
    "    end = activity.get_timeline()[-1].end\n",
    "    return (start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d528106a-2802-4a62-966d-91cf20725c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_and_save(index, samples):\n",
    "    audios = []\n",
    "    metas = []\n",
    "    prev_end = 0\n",
    "    \n",
    "    for idx, row in samples.iterrows():\n",
    "        audio = load_audio(clips_path / row[\"path\"])\n",
    "        timings = measure_sample(audio.to(device))\n",
    "        if timings is None:\n",
    "            print(f\"{index} has no voice detected. Skipping\")\n",
    "            continue\n",
    "        audios += [audio]\n",
    "        metas += [(row[\"client_id\"][:6], row[\"sentence\"], timings[0] + prev_end / default_sample_rate, timings[1] + prev_end / default_sample_rate)]\n",
    "        prev_end += audio.size(1)\n",
    "\n",
    "    concat_audio = torch.concat(audios, 1)\n",
    "    rel_path =f\"clips/{index}.wav\"\n",
    "    abs_path = output_path / rel_path\n",
    "    abs_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torchaudio.save(abs_path, concat_audio, default_sample_rate)\n",
    "\n",
    "    labels = [{\"start\": start, \"end\": end, \"labels\": [f\"speaker-{speaker}\"]} for speaker, _, start, end in metas]\n",
    "    transcription = [sentence for _, sentence, _, _ in metas]\n",
    "    \n",
    "    return {\"audio\": rel_path, \"labels\": labels, \"transcription\": transcription}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e3d3270-0fc3-4c85-b6bc-a9ab79983885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lemon/Documents/workspace/ai_meeting_notes/.venv/lib64/python3.12/site-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
      "It can be re-enabled by calling\n",
      "   >>> import torch\n",
      "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
      "   >>> torch.backends.cudnn.allow_tf32 = True\n",
      "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file number 0\n",
      "Creating file number 1\n",
      "Creating file number 2\n",
      "Creating file number 3\n",
      "Creating file number 4\n",
      "Creating file number 5\n",
      "Creating file number 6\n",
      "Creating file number 7\n",
      "Creating file number 8\n",
      "Creating file number 9\n"
     ]
    }
   ],
   "source": [
    "metas = []\n",
    "\n",
    "for i in range(output_count):\n",
    "    print(f\"Creating file number {i}\")\n",
    "\n",
    "    clients = files[\"client_id\"].sample(clients_per_output)\n",
    "\n",
    "    rows = files[files[\"client_id\"].isin(clients)]\n",
    "    rows = rows.sample(min(len(rows), output_sample_count))\n",
    "\n",
    "    metas += [concat_and_save(i, rows)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8173eccd-231d-49e9-a43a-1ca20f369271",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(metas, open(output_path / \"labels.json\", \"w\"), ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-notes",
   "language": "python",
   "name": "ai-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
